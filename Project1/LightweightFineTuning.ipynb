{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "- PEFT technique: Layer-wise Freezing, selectively freezing certain layers of the model and only training a few upper or final layers. Specifically, we are retraining the classification head, which has not been frozen.\n",
    "\n",
    "- Model: DistilBERT is a smaller, faster, cheaper, and lighter version of BERT. It retains 97% of BERT's language understanding capacity while being 60% faster and 40% smaller. It's well-suited for sequence classification tasks and is efficient for lightweight fine-tuning.\n",
    "\n",
    "- Evaluation approach: \n",
    "    - Accuracy: Measures the proportion of correct predictions.\n",
    "        \n",
    "    - Precision, Recall, and F1-Score: These metrics provide detailed insights into the model's performance for each class. Precision is the accuracy of positive predictions, recall is the ability to find all relevant cases, and F1-score balances both.\n",
    "        \n",
    "    - Confusion Matrix: Shows the distribution of correct and incorrect predictions across all classes.\n",
    "\n",
    "- Fine-tuning dataset: The AG News dataset is a popular text classification dataset that is used to categorize news articles into one of four classes: World, Sports, Business, and Sci/Tech. It's a balanced dataset with a manageable size for lightweight fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "# Load the DistilBERT model and tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224eb26b149e4cb69e75cd3bb5171d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452a356914e74d23ab08359adcfa1c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset('ag_news')\n",
    "dataset['train'] = dataset['train'].shuffle(seed=23).select(range(5000))\n",
    "dataset['test'] = dataset['test'].shuffle(seed=23).select(range(1000))\n",
    "print(f\"dataset:\\n{dataset}\")\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(f\"encoded_dataset:\\n{encoded_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a50e6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "small_test_dataset = dataset['test'].select(range(500))\n",
    "test_texts = small_test_dataset['text']\n",
    "true_labels = small_test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b61cf711",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict(test_texts).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "69aca781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.92      0.09      0.16       123\n",
      "      Sports       0.00      0.00      0.00       122\n",
      "    Business       0.21      0.26      0.23       136\n",
      "    Sci/Tech       0.31      0.82      0.45       119\n",
      "\n",
      "    accuracy                           0.29       500\n",
      "   macro avg       0.36      0.29      0.21       500\n",
      "weighted avg       0.36      0.29      0.21       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(true_labels, predicted_labels, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1fdf5",
   "metadata": {},
   "source": [
    "The model's accuracy is 29%, with poor performance overall. The World category has a high precision of 92% but very low recall, while the Sports category performed the worst with zero precision, recall, and f1-score. The Sci/Tech category did relatively better with a recall of 82%, but its precision is still low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5f5a7630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11  0 28 84]\n",
      " [ 0  0 87 35]\n",
      " [ 1  0 36 99]\n",
      " [ 0  0 22 97]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mx = confusion_matrix(true_labels, predicted_labels)\n",
    "print(f\"Confusion Matrix:\\n{confusion_mx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1cb85",
   "metadata": {},
   "source": [
    "The confusion matrix shows that the model struggled with accurate predictions. The model correctly identified 11 instances of the first class but misclassified many others, particularly confusing them with the fourth class. The second class had no correct predictions, with most being misclassified as the third or fourth class. The third and fourth classes had better performance but still faced significant misclassifications, mainly within each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5537b0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
    "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a1a9fd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=4, bias=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0839219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/categorize_news\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371939</td>\n",
       "      <td>0.893000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/categorize_news/checkpoint-157 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=157, training_loss=0.4316483515842705, metrics={'train_runtime': 68.8347, 'train_samples_per_second': 72.638, 'train_steps_per_second': 2.281, 'total_flos': 456666597240000.0, 'train_loss': 0.4316483515842705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.370482474565506,\n",
       " 'eval_accuracy': 0.889,\n",
       " 'eval_runtime': 5.8344,\n",
       " 'eval_samples_per_second': 171.396,\n",
       " 'eval_steps_per_second': 5.485,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9c3e3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model/fine_tuned_model')\n",
    "#tokenizer.save_pretrained('./model/fine_tuned_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c6cde9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained('./model/fine_tuned_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = dataset['test']['text']\n",
    "true_labels = dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.89      0.87      0.88       241\n",
      "      Sports       0.94      1.00      0.97       253\n",
      "    Business       0.88      0.84      0.86       256\n",
      "    Sci/Tech       0.86      0.86      0.86       250\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = predict(test_texts).cpu().numpy()\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4dee6dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[210  13   9   9]\n",
      " [  0 252   1   0]\n",
      " [ 14   1 215  26]\n",
      " [ 13   2  19 216]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mx = confusion_matrix(true_labels, predicted_labels)\n",
    "print(f\"Confusion Matrix:\\n{confusion_mx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78418559",
   "metadata": {},
   "source": [
    "The confusion matrix shows that the model correctly predicted most instances for each class but had some misclassifications. Overall, it performed best on the second class with only one misclassification and worst on the fourth class with more errors across the other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b02b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The initial model, without any fine-tuning, generated poor results. However, after fine-tuning the model, its performance significantly improved. The fine-tuned model achieved a much higher accuracy of 89%, with high precision and recall across all categories.\n",
    "\n",
    "This demonstrates the importance of fine-tuning in enhancing the model's reliability in classifying different categories, proving that fine-tuning can significantly elevate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9952778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
